[{"content":"Disclaimer All information i write is of my own opinion, and is not reflective of AWS or Amazon.\nWhy Data replication is the process of making multiple copies of your data and storing this data at multiple locations to improve their accessibility across the network.\nSome situations which you may want to replicate your data may include (but are not limited to):\n Data flowing from your application to your database needs to be replicated to a data lake or data warehouse You\u0026rsquo;d like to replicate data from a database to a secondary database location for Disaster Recovery (DR) processes or to perform Data Analytics  How Common database technologies today either have built-in capabilities, or use third-party tools to accomplish data replication. While Oracle and Microsoft SQL actively support data replication, some other technologies (MySQL / Postgres) may not include this feature out of the box.\nI hope the following gives you some ideas on how you can architect methods to replicate data, both before and after the data hits your data storage solution.\n1. Replicate the data before the data storage layer The first method is potentially the easiest - and that is to replicate the data BEFORE the data storage layer.\nPros:\n By replicating and handling the data before it reaches the data storage medium, you can fan-out the data before managing or changing the configuration of the database.  Cons:\n You must ensure that the application or processing of this data is strongly consistent, because you are now introducing a potential failure link You are responsible to write the code that handles the data that is to be replicated to different mediums.   Data Replication - Before the storage layer \n2. Utilise Read Replica(s) You may be in a situation where you\u0026rsquo;d rather not handle the data replication, and prefer to rely on read replicas to peform data replication. The benefits of this approach is that by creating a read replica, you can offset read-heavy workloads to the additional copy of your data, reducing the load on your primary instance.\nPros:\n Database managed feature, so no need to write application logic Can take advantage of write or read replicas based on workflow requirements Read Replicas can be promoted to a master node in the event of a failure  Cons:\n Increased cost due to secondary instance Homogenous method of replicating data, you\u0026rsquo;ll have to introduce a new method or service to go mysql \u0026ndash;\u0026gt; Postgres for example   Data Replication - Read Replica \n3. Aurora Data Activity Streams If you are using Amazon Aurora Database, you can utilise the Database Activity Streams feature to provide near real-time streams of activities within the relational database.\nDatabase Activity Streams are supported for both MySQL and PostgreSQL.\n4. Amazon Data Migration Serice (DMS) In some instances, you may need to perform a heterogeneous data replication between different database engines, such as MySQL to Postgres. The challenges involved with this may prove to be frustrating as you have to build and code your own DB engine translation, or you could use a service such as Amazon Data Migration Service (DMS).\nYou could also use DMS as part of your data lake workflow by connecting DMS to your database, and then using AWS Glue to format the data. Ideally you want to always keep a copy of the original data before formatting, as part of your data retention and data lineage strategy.\nThis blog highlights this strategy.\n Data Replication - DMS \n5. Third Party Tools \u0026amp; Binlog Replication You may find yourself in a situation where all of the above is not viable, and you prefer an open source method of data replication. You can look at utilising Debezium or look for other Change Data Capture (CDC) tools.\n","date":"2021-02-04T14:08:03+11:00","image":"http://tsengsy.github.io/p/methods-to-tackle-data-replication/hdd_hu3d03a01dcc18bc5be0e67db3d8d209a6_364101_120x120_fill_q75_box_smart1.jpeg","permalink":"http://tsengsy.github.io/p/methods-to-tackle-data-replication/","title":"Different methods to tackle Change Data Capture (CDC) \u0026 Data Replication"},{"content":"Disclaimer All information i write is of my own opinion, and is not reflective of AWS or Amazon.\nLast year, i\u0026rsquo;ve been in a discussion with a customer about how to manage SSL certificates when you use an Application Load Balancer (ALB). Currently, as of January 07 2021, ALB\u0026rsquo;s have a maximum of 25 SSL certificates.\nHowever, what happens if you want to host or support more than 25 certificates for your business? Let\u0026rsquo;s work through how we can resolve this.\nThe simplest solution to start with might be to use Rule-Based Routing on Application Load Balancers. This allows you utilise the same 443 port, but handle path or hostname based routing.\n Rule Based Routing on ALB \n However, lets assume you have the following infrastructure and you are hosting a website for hundreds of customers.\n Example infrastructure \nThe difficulties you are experiencing are that for each custom domain, you need an ACM mapping and also there is the 25 limit per ELB. You don\u0026rsquo;t want to keep creating ALBs just for a group of 25 certificates due to cost.\nThe First Method The first method we can use is to implement CloudFront in-between Elastic beanstalk and Route53. This means that you can have as many CloudFront distributions that you would like to have, and assign HTTPS on CloudFront, setup ACM and remove the need for SSL to be done on the load balancer. However you can only associate a maximum of 1 SSL/TLS cert with each CloudFront Distribution.\n First Solution \nThe Second Method The second method is to implement your own proxy (HAproxy / NGINX). You do have to consider management overhead, plus you cannot take advantage of ACM to perform automated certificate rotation, as ACM only works in conjunction with select AWS services. You’d also want to script the certificate rotation process and have to manage scaling the instance. This is one of the most commonly used methods to work around the 25 cert limit at present.\n Second Solution \nThe Third Method The third method is more expensive, but you create multiple ELBs which then route to your elastic beanstalk. I’d be estimating the cost between this solution and the one above if you were going down the SSL proxy / gateway route. Looking to scale this you’d be adding n+1 LB, up to the maximum of 50 load balancers per region.\n Third Solution \n Personally in my opinion, it makes more sense to start with investigating whether the CloudFront route is a viable option, as you really want to remove the hassle of manually rotating certificates. You should remember to also setup an ALIAS record instead of CNAME (reduces the cost), and you could also encrypt the connection between CloudFront and your Elastic Beanstalk endpoint. The encryption is optional as it depends on your security standpoint and whether some websites needs internal encryption.\nThere is also a maximum of 200 CloudFront distributions per account, but this can be increased with a support request.\n Any questions or concerns? Feel free to reach me at yiyang.tseng@gmail.com","date":"2021-01-07T10:58:39+11:00","image":"http://tsengsy.github.io/p/ssl-limits-alb/stock_world_hu3d03a01dcc18bc5be0e67db3d8d209a6_662934_120x120_fill_q75_box_smart1.jpg","permalink":"http://tsengsy.github.io/p/ssl-limits-alb/","title":"Tackling 25 SSL certificate limits for AWS Application Load Balancers"}]